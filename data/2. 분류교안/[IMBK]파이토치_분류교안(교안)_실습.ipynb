{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61P2E1eTnNPw"
   },
   "source": [
    "### 1. DNN을 통한 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQwo7cWYnSCv"
   },
   "source": [
    "#### ※ 앞으로 궁금할 상식: 분류 학습에서 왜 Cross Entropy를 사용할 때, output layer에 activation function을 안쓰는걸까?\n",
    "* (Binary) Cross Entropy에 이미 activation(예: sigmoid, softmax)이 내재되어 있어, 1차로 output의 features(Not categorical)를 activation 해준 후에, 2차로 CE loss 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0j-sh9BnTUe"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbvVAL-7P941"
   },
   "source": [
    "### 2. wine.csv 데이터셋 기반 DNN 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBRvFjdpQNW2",
    "outputId": "c3524113-faac-4ad3-960c-16d42ce359c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0722\n",
      "Epoch 2/10, Loss: 0.1342\n",
      "Epoch 3/10, Loss: 0.0167\n",
      "Epoch 4/10, Loss: 0.0266\n",
      "Epoch 5/10, Loss: 0.0045\n",
      "Epoch 6/10, Loss: 0.0111\n",
      "Epoch 7/10, Loss: 0.0039\n",
      "Epoch 8/10, Loss: 0.0046\n",
      "Epoch 9/10, Loss: 0.0054\n",
      "Epoch 10/10, Loss: 0.0381\n",
      "Accuracy: 0.9977\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "# 화이트 와인(0)인지 레드와인(1)인지\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) # 2차원으로 만들어야함. 모델 예측은 2차원으로 나오기 때문\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 8)\n",
    "        self.layer2 = nn.Linear(8, 16)\n",
    "        self.layer3 = nn.Linear(16, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.layer5 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = DNN(input_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = torch.sigmoid(y_pred) # 시그모이드를 통과해서 확률값 구함\n",
    "        y_pred = (y_pred > 0.5).float()\n",
    "        correct += (y_pred == y_batch).sum().item() # 맞춘 값드를 더한 다음에\n",
    "        total += y_batch.size(0) # 해당 미니배치 데이터 셋 갯수를 구해놓고\n",
    "\n",
    "    accuracy = correct / total # 맞춘것들 누적 / 미니배치 데이터셋 갯수 누적\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y16XpWBQCI3r"
   },
   "source": [
    "### 실습) wine.csv 데이터셋 기반 DNN 이진분류를 누군가 코드를 망쳐놨다. 고쳐보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWjUjwfJCKie",
    "outputId": "d9eeb538-6659-4006-ac88-222979119f25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5461\n",
      "Epoch 2/10, Loss: 0.6282\n",
      "Epoch 3/10, Loss: 0.6238\n",
      "Epoch 4/10, Loss: 0.4975\n",
      "Epoch 5/10, Loss: 0.3440\n",
      "Epoch 6/10, Loss: 0.5792\n",
      "Epoch 7/10, Loss: 0.3218\n",
      "Epoch 8/10, Loss: 0.1444\n",
      "Epoch 9/10, Loss: 0.2250\n",
      "Epoch 10/10, Loss: 0.0850\n",
      "Accuracy: 0.9946\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "# 화이트 와인(0)인지 레드와인(1)인지\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) # 2차원으로 만들어야함. 모델 예측은 2차원으로 나오기 때문\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 8)\n",
    "        self.bn1 = nn.BatchNorm1d(8)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.layer2 = nn.Linear(8, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.layer3 = nn.Linear(16, 16)\n",
    "        self.bn3 = nn.BatchNorm1d(16)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.bn4 = nn.BatchNorm1d(8)\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "        self.layer5 = nn.Linear(8, 1)  # 수정\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = DNN(input_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()           # 수정\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = torch.sigmoid(y_pred)   # 시그모이드를 통과해서 확률값 구함  # 수정\n",
    "        y_pred = (y_pred > 0.5).float()# 수정\n",
    "        correct += (y_pred == y_batch).sum().item()# 수정\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    accuracy =correct /total# 수정\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl7kpUWxx0r5"
   },
   "source": [
    "### 3. wine.csv 데이터셋 기반 DNN 이진분류\n",
    "\n",
    "1). accracy\n",
    "\n",
    "2). f1_score\n",
    "\n",
    "3). confusion matrix\n",
    "\n",
    "4). precision\n",
    "\n",
    "5). recall\n",
    "\n",
    "을 출력해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IfabJPiyF4O",
    "outputId": "0c832656-2220-443e-feae-1d76a0176fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0397\n",
      "Epoch 2/10, Loss: 0.0018\n",
      "Epoch 3/10, Loss: 0.0048\n",
      "Epoch 4/10, Loss: 0.0016\n",
      "Epoch 5/10, Loss: 0.3333\n",
      "Epoch 6/10, Loss: 0.0050\n",
      "Epoch 7/10, Loss: 0.0020\n",
      "Epoch 8/10, Loss: 0.0111\n",
      "Epoch 9/10, Loss: 0.0023\n",
      "Epoch 10/10, Loss: 0.0038\n",
      "Accuracy: 0.9977\n",
      "F1 Score: 0.9953\n",
      "Confusion Matrix:\n",
      "[[980   0]\n",
      " [  3 317]]\n",
      "Precision: 1.0000\n",
      "Recall: 0.9906\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "# 화이트 와인(0)인지 레드와인(1)인지\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1) # 2차원으로 만들어야함. 모델 예측은 2차원으로 나오기 때문\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 8)\n",
    "        self.layer2 = nn.Linear(8, 16)\n",
    "        self.layer3 = nn.Linear(16, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.layer5 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = DNN(input_dim)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = torch.sigmoid(y_pred)  # 시그모이드를 통과해서 확률값 구함\n",
    "        y_pred = (y_pred > 0.5).float()  # 0.5를 기준으로 이진화\n",
    "        # 0또는 1로 최종 결과 값으로 바뀜\n",
    "\n",
    "        y_preds.extend(y_pred.view(-1).tolist()) # 예측값 1d리스트 타입으로 바꿈\n",
    "        y_trues.extend(y_batch.view(-1).tolist())# 예측값 1d리스트 타입으로 바꿈\n",
    "\n",
    "    # 성능 지표 계산\n",
    "    accuracy = accuracy_score(y_trues, y_preds)\n",
    "    f1 = f1_score(y_trues, y_preds)\n",
    "    cm = confusion_matrix(y_trues, y_preds)\n",
    "    precision = precision_score(y_trues, y_preds)\n",
    "    recall = recall_score(y_trues, y_preds)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Confusion Matrix:\\n{cm}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQq0UII-4-Pt"
   },
   "source": [
    "### 4. wine.csv 데이터셋 기반 DNN 이진분류 - 클래스 불균형 해결\n",
    "\n",
    "SMOTE를 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "PnmOIYRu5CLK",
    "outputId": "3b8ba476-1e25-42ca-aea8-163f9d9790ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "class\n",
       "0    4898\n",
       "1    1599\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "\n",
    "df = pd.read_csv('wine.csv')\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lehX3XKk5LMj",
    "outputId": "1bf757d5-9940-44ae-82db-8816a3f7f001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTE: \n",
      "0    3918\n",
      "1    3918\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values   # 화이트 와인: 0, 레드 와인: 1\n",
    "\n",
    "# 데이터셋 분리 (훈련 및 테스트 셋)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# SMOTE 적용\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# SMOTE 적용 후 클래스 분포 확인\n",
    "print(f\"After SMOTE: \\n{pd.Series(y_train_resampled).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KfLyHaqf5bFe",
    "outputId": "c657c3b8-148c-4597-e22a-22692472f6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.0247\n",
      "Epoch 2/10, Loss: 0.0099\n",
      "Epoch 3/10, Loss: 0.1199\n",
      "Epoch 4/10, Loss: 0.0085\n",
      "Epoch 5/10, Loss: 0.0046\n",
      "Epoch 6/10, Loss: 0.0039\n",
      "Epoch 7/10, Loss: 0.0038\n",
      "Epoch 8/10, Loss: 0.0026\n",
      "Epoch 9/10, Loss: 0.0056\n",
      "Epoch 10/10, Loss: 0.0041\n",
      "Accuracy: 0.9985\n",
      "F1 Score: 0.9969\n",
      "Confusion Matrix:\n",
      "[[980   0]\n",
      " [  2 318]]\n",
      "Precision: 1.0000\n",
      "Recall: 0.9938\n"
     ]
    }
   ],
   "source": [
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 데이터를 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).unsqueeze(1)  # 2차원으로 변환\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# 데이터셋과 데이터로더 준비\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# DNN 모델 정의\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 8)\n",
    "        self.layer2 = nn.Linear(8, 16)\n",
    "        self.layer3 = nn.Linear(16, 16)\n",
    "        self.layer4 = nn.Linear(16, 8)\n",
    "        self.layer5 = nn.Linear(8, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.layer5(x)\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = DNN(input_dim)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_preds = []\n",
    "    y_trues = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = torch.sigmoid(y_pred)  # 시그모이드를 통과해서 확률값 구함\n",
    "        y_pred = (y_pred > 0.5).float()  # 0.5를 기준으로 이진화\n",
    "        y_preds.extend(y_pred.view(-1).tolist())\n",
    "        y_trues.extend(y_batch.view(-1).tolist())\n",
    "\n",
    "    # 성능 지표 계산\n",
    "    accuracy = accuracy_score(y_trues, y_preds)\n",
    "    f1 = f1_score(y_trues, y_preds)\n",
    "    cm = confusion_matrix(y_trues, y_preds)\n",
    "    precision = precision_score(y_trues, y_preds)\n",
    "    recall = recall_score(y_trues, y_preds)\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Confusion Matrix:\\n{cm}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOZNafnxQRy0"
   },
   "source": [
    "### 같이해보는 실습) 타이타닉 데이터셋으로 MLP 모델을 이용하여 생존분류를 해보자  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "s0NUUYZzgprr",
    "outputId": "72da2fe7-4b28-413d-a845-eeab33b2ecbb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "Survived\n",
       "0    424\n",
       "1    288\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qcv4VMCtXpDz",
    "outputId": "371bd926-23b2-446d-fb7c-8616a43afadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.6882\n",
      "Epoch 2/50, Loss: 0.6864\n",
      "Epoch 3/50, Loss: 0.6872\n",
      "Epoch 4/50, Loss: 0.6882\n",
      "Epoch 5/50, Loss: 0.6981\n",
      "Epoch 6/50, Loss: 0.6956\n",
      "Epoch 7/50, Loss: 0.6769\n",
      "Epoch 8/50, Loss: 0.6683\n",
      "Epoch 9/50, Loss: 0.6590\n",
      "Epoch 10/50, Loss: 0.6900\n",
      "Epoch 11/50, Loss: 0.6570\n",
      "Epoch 12/50, Loss: 0.6741\n",
      "Epoch 13/50, Loss: 0.6906\n",
      "Epoch 14/50, Loss: 0.6831\n",
      "Epoch 15/50, Loss: 0.6441\n",
      "Epoch 16/50, Loss: 0.6691\n",
      "Epoch 17/50, Loss: 0.6577\n",
      "Epoch 18/50, Loss: 0.5907\n",
      "Epoch 19/50, Loss: 0.6342\n",
      "Epoch 20/50, Loss: 0.5972\n",
      "Epoch 21/50, Loss: 0.6541\n",
      "Epoch 22/50, Loss: 0.5426\n",
      "Epoch 23/50, Loss: 0.5299\n",
      "Epoch 24/50, Loss: 0.4771\n",
      "Epoch 25/50, Loss: 0.6056\n",
      "Epoch 26/50, Loss: 0.5816\n",
      "Epoch 27/50, Loss: 0.4814\n",
      "Epoch 28/50, Loss: 0.4129\n",
      "Epoch 29/50, Loss: 0.5111\n",
      "Epoch 30/50, Loss: 0.5807\n",
      "Epoch 31/50, Loss: 0.6035\n",
      "Epoch 32/50, Loss: 0.4491\n",
      "Epoch 33/50, Loss: 0.3604\n",
      "Epoch 34/50, Loss: 0.6010\n",
      "Epoch 35/50, Loss: 0.6182\n",
      "Epoch 36/50, Loss: 0.5537\n",
      "Epoch 37/50, Loss: 0.4614\n",
      "Epoch 38/50, Loss: 0.2545\n",
      "Epoch 39/50, Loss: 0.5517\n",
      "Epoch 40/50, Loss: 0.6339\n",
      "Epoch 41/50, Loss: 0.5857\n",
      "Epoch 42/50, Loss: 0.3617\n",
      "Epoch 43/50, Loss: 0.3925\n",
      "Epoch 44/50, Loss: 0.5045\n",
      "Epoch 45/50, Loss: 0.3425\n",
      "Epoch 46/50, Loss: 0.5832\n",
      "Epoch 47/50, Loss: 0.5823\n",
      "Epoch 48/50, Loss: 0.4346\n",
      "Epoch 49/50, Loss: 0.3969\n",
      "Epoch 50/50, Loss: 0.5300\n",
      "Accuracy: 0.8322\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "df.dropna(inplace=True) # nan값이면 삭제하고 실제로 반영\n",
    "\n",
    "# 범주형 컬럼 라벨 인코더로 숫자로 바꿈\n",
    "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
    "df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
    "\n",
    "# 정답값 빼고 X정의\n",
    "X = df.drop('Survived', axis=1).values\n",
    "\n",
    "# 정답값만 가져오기\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 16)\n",
    "        self.layer2 = nn.Linear(16, 32)\n",
    "        self.layer3 = nn.Linear(32, 64)\n",
    "        self.layer4 = nn.Linear(64, 64)\n",
    "        self.layer5 = nn.Linear(64, 32)\n",
    "        self.layer6 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))\n",
    "        x = self.relu(self.layer4(x))\n",
    "        x = self.relu(self.layer5(x))\n",
    "        x = self.sigmoid(self.layer6(x))\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLP(input_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = (y_pred > 0.5).float()\n",
    "        correct += (y_pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBUDTksI5sZ5"
   },
   "source": [
    "### 5. 타이타닉 데이터셋으로 MLP 모델을 이용하여 생존분류를 해보자 - 클래스 불균형 해결\n",
    "\n",
    "SMOTENC를 사용해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M7N1EOAc5-CV",
    "outputId": "90f5e99d-e8a0-4150-88d7-011acac360ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived\n",
      "0    424\n",
      "1    288\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "df = pd.read_csv('titanic.csv')\n",
    "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 각 클래스가 몇개인지 보자\n",
    "print(df['Survived'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VwlnqHZ5s_F",
    "outputId": "47adaeb6-1a77-4e00-ba10-e3a032a6d7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.5304\n",
      "Epoch 2/50, Loss: 0.6868\n",
      "Epoch 3/50, Loss: 0.6178\n",
      "Epoch 4/50, Loss: 0.4012\n",
      "Epoch 5/50, Loss: 0.6851\n",
      "Epoch 6/50, Loss: 0.7146\n",
      "Epoch 7/50, Loss: 0.4431\n",
      "Epoch 8/50, Loss: 0.7980\n",
      "Epoch 9/50, Loss: 0.1505\n",
      "Epoch 10/50, Loss: 0.0982\n",
      "Epoch 11/50, Loss: 0.2655\n",
      "Epoch 12/50, Loss: 0.9326\n",
      "Epoch 13/50, Loss: 0.3862\n",
      "Epoch 14/50, Loss: 0.1604\n",
      "Epoch 15/50, Loss: 0.2623\n",
      "Epoch 16/50, Loss: 0.1762\n",
      "Epoch 17/50, Loss: 0.2672\n",
      "Epoch 18/50, Loss: 0.6393\n",
      "Epoch 19/50, Loss: 0.5768\n",
      "Epoch 20/50, Loss: 0.8420\n",
      "Epoch 21/50, Loss: 0.5342\n",
      "Epoch 22/50, Loss: 0.2515\n",
      "Epoch 23/50, Loss: 0.6670\n",
      "Epoch 24/50, Loss: 0.9592\n",
      "Epoch 25/50, Loss: 0.4756\n",
      "Epoch 26/50, Loss: 0.3280\n",
      "Epoch 27/50, Loss: 0.3350\n",
      "Epoch 28/50, Loss: 0.6400\n",
      "Epoch 29/50, Loss: 0.6416\n",
      "Epoch 30/50, Loss: 0.1037\n",
      "Epoch 31/50, Loss: 0.4178\n",
      "Epoch 32/50, Loss: 0.5010\n",
      "Epoch 33/50, Loss: 0.3241\n",
      "Epoch 34/50, Loss: 0.0694\n",
      "Epoch 35/50, Loss: 0.2882\n",
      "Epoch 36/50, Loss: 0.6777\n",
      "Epoch 37/50, Loss: 0.1137\n",
      "Epoch 38/50, Loss: 0.5520\n",
      "Epoch 39/50, Loss: 0.5354\n",
      "Epoch 40/50, Loss: 0.7614\n",
      "Epoch 41/50, Loss: 0.3878\n",
      "Epoch 42/50, Loss: 0.1101\n",
      "Epoch 43/50, Loss: 0.0985\n",
      "Epoch 44/50, Loss: 0.3114\n",
      "Epoch 45/50, Loss: 0.4893\n",
      "Epoch 46/50, Loss: 0.4429\n",
      "Epoch 47/50, Loss: 0.4582\n",
      "Epoch 48/50, Loss: 0.2117\n",
      "Epoch 49/50, Loss: 0.6828\n",
      "Epoch 50/50, Loss: 0.6557\n"
     ]
    }
   ],
   "source": [
    "# 범주형 변수 인코딩\n",
    "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
    "df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
    "\n",
    "X = df.drop('Survived', axis=1).values\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 범주형 변수 인덱스 (Pclass, Sex, Embarked)\n",
    "categorical_features = [0, 1, 4]\n",
    "\n",
    "# SMOTE-NC 적용\n",
    "smote_nc = SMOTENC(categorical_features=categorical_features, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# 범주형과 수치형 변수를 고려하여 , 클래스 균형에 맞게 데이터를 upsampleing 한 결과\n",
    "X_train_resampled, y_train_resampled = smote_nc.fit_resample(X_train, y_train)\n",
    "\n",
    "# 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_resampled = scaler.fit_transform(X_train_resampled)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 텐서변환\n",
    "X_train_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_resampled, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# MLP 모델 정의\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.sigmoid(self.layer3(x))\n",
    "        return x\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "model = MLP(input_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 모델 학습\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred = (y_pred > 0.5).float()\n",
    "        correct += (y_pred == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    accuracy = correct / total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9WwLlFqP9zC"
   },
   "source": [
    "### 3. iris.csv 데이터셋 기반 MLP 다중분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wGK9UPDVl8hX",
    "outputId": "f13501f9-3440-43a1-be89-01e0221daee4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv('iris.csv')\n",
    "df['variety'] = LabelEncoder().fit_transform(df['variety'])\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xn0yhhmJfkhE",
    "outputId": "b0caed66-7c19-410d-a519-2728e7766657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.9083, Val Loss: 0.9083\n",
      "Epoch 2/50, Loss: 0.7948, Val Loss: 0.7948\n",
      "Epoch 3/50, Loss: 0.6974, Val Loss: 0.6974\n",
      "Epoch 4/50, Loss: 0.6142, Val Loss: 0.6142\n",
      "Epoch 5/50, Loss: 0.5449, Val Loss: 0.5449\n",
      "Epoch 6/50, Loss: 0.4897, Val Loss: 0.4897\n",
      "Epoch 7/50, Loss: 0.4433, Val Loss: 0.4433\n",
      "Epoch 8/50, Loss: 0.4057, Val Loss: 0.4057\n",
      "Epoch 9/50, Loss: 0.3740, Val Loss: 0.3740\n",
      "Epoch 10/50, Loss: 0.3470, Val Loss: 0.3470\n",
      "Epoch 11/50, Loss: 0.3251, Val Loss: 0.3251\n",
      "Epoch 12/50, Loss: 0.3053, Val Loss: 0.3053\n",
      "Epoch 13/50, Loss: 0.2886, Val Loss: 0.2886\n",
      "Epoch 14/50, Loss: 0.2725, Val Loss: 0.2725\n",
      "Epoch 15/50, Loss: 0.2584, Val Loss: 0.2584\n",
      "Epoch 16/50, Loss: 0.2437, Val Loss: 0.2437\n",
      "Epoch 17/50, Loss: 0.2294, Val Loss: 0.2294\n",
      "Epoch 18/50, Loss: 0.2156, Val Loss: 0.2156\n",
      "Epoch 19/50, Loss: 0.2011, Val Loss: 0.2011\n",
      "Epoch 20/50, Loss: 0.1862, Val Loss: 0.1862\n",
      "Epoch 21/50, Loss: 0.1710, Val Loss: 0.1710\n",
      "Epoch 22/50, Loss: 0.1590, Val Loss: 0.1590\n",
      "Epoch 23/50, Loss: 0.1471, Val Loss: 0.1471\n",
      "Epoch 24/50, Loss: 0.1392, Val Loss: 0.1392\n",
      "Epoch 25/50, Loss: 0.1305, Val Loss: 0.1305\n",
      "Epoch 26/50, Loss: 0.1218, Val Loss: 0.1218\n",
      "Epoch 27/50, Loss: 0.1151, Val Loss: 0.1151\n",
      "Epoch 28/50, Loss: 0.1087, Val Loss: 0.1087\n",
      "Epoch 29/50, Loss: 0.1041, Val Loss: 0.1041\n",
      "Epoch 30/50, Loss: 0.1009, Val Loss: 0.1009\n",
      "Epoch 31/50, Loss: 0.0984, Val Loss: 0.0984\n",
      "Epoch 32/50, Loss: 0.0948, Val Loss: 0.0948\n",
      "Epoch 33/50, Loss: 0.0922, Val Loss: 0.0922\n",
      "Epoch 34/50, Loss: 0.0886, Val Loss: 0.0886\n",
      "Epoch 35/50, Loss: 0.0870, Val Loss: 0.0870\n",
      "Epoch 36/50, Loss: 0.0851, Val Loss: 0.0851\n",
      "Epoch 37/50, Loss: 0.0841, Val Loss: 0.0841\n",
      "Epoch 38/50, Loss: 0.0831, Val Loss: 0.0831\n",
      "Epoch 39/50, Loss: 0.0823, Val Loss: 0.0823\n",
      "Epoch 40/50, Loss: 0.0820, Val Loss: 0.0820\n",
      "Epoch 41/50, Loss: 0.0817, Val Loss: 0.0817\n",
      "Epoch 42/50, Loss: 0.0817, Val Loss: 0.0817\n",
      "Epoch 43/50, Loss: 0.0811, Val Loss: 0.0811\n",
      "Epoch 44/50, Loss: 0.0798, Val Loss: 0.0798\n",
      "Epoch 45/50, Loss: 0.0789, Val Loss: 0.0789\n",
      "Epoch 46/50, Loss: 0.0790, Val Loss: 0.0790\n",
      "Epoch 47/50, Loss: 0.0788, Val Loss: 0.0788\n",
      "Epoch 48/50, Loss: 0.0793, Val Loss: 0.0793\n",
      "Epoch 49/50, Loss: 0.0792, Val Loss: 0.0792\n",
      "Epoch 50/50, Loss: 0.0793, Val Loss: 0.0793\n",
      "Accuracy: 0.9667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "df = pd.read_csv('iris.csv')\n",
    "df['variety'] = LabelEncoder().fit_transform(df['variety'])\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "'''\n",
    "# 방법 1: nn.Sequential()과 add_module 사용\n",
    "\n",
    "model = nn.Sequential()\n",
    "model.add_module('fc1', nn.Linear(4, 100))\n",
    "model.add_module('relu1', nn.ReLU())\n",
    "model.add_module('fc2', nn.Linear(100, 100))\n",
    "model.add_module('relu2', nn.ReLU())\n",
    "model.add_module('fc3', nn.Linear(100, 3))\n",
    "\n",
    "# 방법 2: nn.Sequential() 안에 직접 레이어 정의\n",
    "model = nn.Sequential(\n",
    "     nn.Linear(4, 100),  # input_layer = 4, hidden_layer1 = 100\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(100, 100),  # hidden_layer2 = 100, hidden_layer3 = 100\n",
    "     nn.ReLU(),\n",
    "     nn.Linear(100, 3)  # hidden_layer3 = 100, output_layer = 3\n",
    "     )\n",
    "'''\n",
    "# 방법 3: nn.Module을 상속하는 클래스 정의\n",
    "class MultiLayerRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerRegression, self).__init__()\n",
    "                                                # X_train이 데이터 X 피처갯수의 shape을 가지는데 거기서 인덱스 1(피처갯수)\n",
    "        self.linear1 = nn.Linear(X_train.shape[1], 100)  # input_layer = X_train.shape[1], hidden_layer1 = 100\n",
    "        self.activate1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(100, 100)  # hidden_layer2 = 100, hidden_layer3 = 100\n",
    "        self.activate2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(100, len(set(y)))  # hidden_layer3 = 100, output_layer = len(set(y))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.linear1(x)\n",
    "        out2 = self.activate1(out1)\n",
    "        out3 = self.linear2(out2)\n",
    "        out4 = self.activate2(out3)\n",
    "        out5 = self.linear3(out4)\n",
    "        return out5\n",
    "\n",
    "model = MultiLayerRegression()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKsG3zdr8ga7"
   },
   "source": [
    "# ★ 실무 실습) 배포 받은 \"신용카드 사용자 연체 예측\"를 통해 자신만의 코드를 작성 해보자\n",
    "\n",
    "[사람의 정보를 넣어서 신용을 분류예측 하는 모델을 만들어 보자!]\n",
    "\n",
    "1. credit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도 (0,1,2)=> 낮을 수록 높은 신용의 신용카드 사용자를 의미함\n",
    "\n",
    "2. 하나의 포트폴리오가 될 수 있도록 시각화와 통계를 자유롭게 작성해보자.\n",
    "3. 이제까지 배운 분류 방법들을 통해 적용해보자.\n",
    "4. 모든 기법을 다 쓰는 것보다는 필요 할 것 같다는 코드만 작성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxmkCT2hqYC0",
    "outputId": "4110843a-fccd-417c-8ab8-6f9e05beb563"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-61b28318da64>:11: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data_train = pd.read_csv('train.csv', sep=',', skiprows=[16002,23976]) # Skip the line before the error\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3b5V6T714bS"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "data_train = pd.read_csv('train.csv', sep=',', skiprows=[16002,23976]) # Skip the line before the error\n",
    "data_train = data_train.drop_duplicates(subset=['index']).reset_index(drop=True)\n",
    "data_train = data_train.fillna('no jop')\n",
    "data_train.drop(columns = ['index'],inplace = True)\n",
    "\n",
    "# 1. Label Encoding 적용 (이미 된 부분이라 가정)\n",
    "label_encoder = LabelEncoder()\n",
    "for column in data_train.columns:\n",
    "    if data_train[column].dtype == 'object':\n",
    "        data_train[column] = label_encoder.fit_transform(data_train[column].astype(str))\n",
    "\n",
    "# 2. 범주형 열 인덱스 확인 (라벨 인코딩된 열)\n",
    "categorical_features = [i for i, col in enumerate(data_train.columns) if data_train[col].dtype == 'int64']\n",
    "\n",
    "# 3. X와 y 설정\n",
    "X = data_train.iloc[:, :-1].values  # 마지막 열 제외\n",
    "y = data_train.iloc[:, -1].values  # 마지막 열\n",
    "\n",
    "# 4. 훈련 데이터와 테스트 데이터 분할 (stratify로 클래스 비율 유지)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 6. 범주형 변수를 제외한 수치형 열에 대해 스케일링 적용\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t317XNQgPwhd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HQnJzG75819K",
    "outputId": "62920b4d-331e-47f1-989b-ab673004b3f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.8637, Val Loss: 0.8630\n",
      "Epoch 2/100, Loss: 0.7020, Val Loss: 0.8394\n",
      "Epoch 3/100, Loss: 0.8914, Val Loss: 0.8260\n",
      "Epoch 4/100, Loss: 0.5469, Val Loss: 0.8213\n",
      "Epoch 5/100, Loss: 1.3262, Val Loss: 0.8236\n",
      "Epoch 6/100, Loss: 0.8121, Val Loss: 0.8236\n",
      "Epoch 7/100, Loss: 0.9670, Val Loss: 0.8311\n",
      "Epoch 8/100, Loss: 1.4244, Val Loss: 0.8263\n",
      "Epoch 9/100, Loss: 0.6869, Val Loss: 0.8239\n",
      "Epoch 10/100, Loss: 0.9400, Val Loss: 0.8196\n",
      "Epoch 11/100, Loss: 0.8772, Val Loss: 0.8212\n",
      "Epoch 12/100, Loss: 0.6154, Val Loss: 0.8249\n",
      "Epoch 13/100, Loss: 0.7475, Val Loss: 0.8233\n",
      "Epoch 14/100, Loss: 0.4033, Val Loss: 0.8208\n",
      "Epoch 15/100, Loss: 0.6894, Val Loss: 0.8209\n",
      "Epoch 16/100, Loss: 0.5647, Val Loss: 0.8227\n",
      "Epoch 17/100, Loss: 0.6337, Val Loss: 0.8258\n",
      "Epoch 18/100, Loss: 0.7233, Val Loss: 0.8361\n",
      "Epoch 19/100, Loss: 0.9564, Val Loss: 0.8288\n",
      "Epoch 20/100, Loss: 0.8152, Val Loss: 0.8386\n",
      "Early stopping triggered\n",
      "Accuracy: 0.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-fbdbe4993a0a>:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "class MultiLayerRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerRegression, self).__init__()\n",
    "                                                # X_train이 데이터 X 피처갯수의 shape을 가지는데 거기서 인덱스 1(피처갯수)\n",
    "        self.linear1 = nn.Linear(X_train.shape[1], 40)\n",
    "        self.activate1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(40, 80)\n",
    "        self.activate2 = nn.ReLU()\n",
    "        self.linear3 = nn.Linear(80, 40)\n",
    "        self.activate3 = nn.ReLU()\n",
    "        self.linear4 = nn.Linear(40, 40)\n",
    "        self.activate4 = nn.ReLU()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.linear5 = nn.Linear(40, len(set(y)))  # hidden_layer3 = 100, output_layer = len(set(y))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1 = self.linear1(x)\n",
    "        out2 = self.activate1(out1)\n",
    "        out3 = self.linear2(out2)\n",
    "        out4 = self.activate2(out3)\n",
    "        out5 = self.linear3(out4)\n",
    "        out6 = self.activate3(out5)\n",
    "        out7 = self.linear4(out6)\n",
    "        out8 = self.activate4(out7)\n",
    "        out9 = self.linear5(out8)\n",
    "        return out9\n",
    "\n",
    "model = MultiLayerRegression()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "patience = 10\n",
    "best_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        y_pred = model(X_batch)\n",
    "        train_loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(test_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {train_loss.item():.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    if early_stop_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = []\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y03AMzHvjJv8"
   },
   "source": [
    "### 4. 깃허브 울렁증 격파하기\n",
    "\n",
    "스토리: 누군가 나에게 분류에는 어떤 모델이 좋다고 써보라고 했다. 그 모델이 뭔진 모르겠다. 그럼에도 불구하고 갖다 써보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcpQ2J-DjOQv"
   },
   "source": [
    "### 1) TabNet 스크립트 버전\n",
    "\n",
    "---\n",
    "https://github.com/huangyz0918/tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2JPlpic7zzu",
    "outputId": "5f6733a6-a1bf-408b-ed11-0cd1f93a3584"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'tabnet'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfTrdG2j73Ov",
    "outputId": "d8ca7330-4de0-4610-db58-f3dc7523cbaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "!pwd # 현재 작업디렉토리의 결로 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "iO9h6jUt9BGB",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f041a617-519d-4f65-9267-4fc729a7bd0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neptune-client==0.9.18 in /usr/local/lib/python3.10/dist-packages (0.9.18)\n",
      "Requirement already satisfied: bravado in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (11.0.3)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (8.1.7)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (1.0.0)\n",
      "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (3.2.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (2.1.4)\n",
      "Requirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (9.4.0)\n",
      "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (2.9.0)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (1.3.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (1.16.0)\n",
      "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (1.8.0)\n",
      "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (3.1.43)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (24.1)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune-client==0.9.18) (2.0.7)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune-client==0.9.18) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune-client==0.9.18) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune-client==0.9.18) (3.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune-client==0.9.18) (2024.8.30)\n",
      "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (6.1.1)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (1.0.8)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (2.8.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (6.0.2)\n",
      "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (3.19.3)\n",
      "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (1.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from bravado->neptune-client==0.9.18) (4.12.2)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client==0.9.18) (1.26.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client==0.9.18) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune-client==0.9.18) (2024.1)\n",
      "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (1.1.0)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (4.23.0)\n",
      "Requirement already satisfied: swagger-spec-validator>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (3.0.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune-client==0.9.18) (5.0.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.5.1->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (0.20.0)\n",
      "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (3.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (0.1.1)\n",
      "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (24.8.0)\n",
      "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.0.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (6.4.4)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado->neptune-client==0.9.18) (2.9.0.20240821)\n"
     ]
    }
   ],
   "source": [
    "!pip install neptune-client==0.9.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yhfRJbS9kxM",
    "outputId": "a485cefa-4862-4b05-980d-c13f1f0b3df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/content/main.py\", line 8, in <module>\n",
      "    from tabnet import TabNet\n",
      "ImportError: cannot import name 'TabNet' from 'tabnet' (unknown location)\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PW2VMc6QRobW",
    "outputId": "f6a83ffe-3b1c-45b1-eb46-ea8b3811ffe7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tabnet'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 33 (delta 11), reused 19 (delta 5), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (33/33), 10.74 MiB | 8.87 MiB/s, done.\n",
      "Resolving deltas: 100% (11/11), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huangyz0918/tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmexF2htRsVh",
    "outputId": "b2f5e075-fc12-4a41-c4e6-c1f2c3b7e92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/tabnet\n"
     ]
    }
   ],
   "source": [
    "%cd tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Y_1oHx9BMcF"
   },
   "outputs": [],
   "source": [
    "!pip install sparsemax==0.1.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4So4AL_ZFhMH",
    "outputId": "cbb7e2cd-a950-4ebc-cf9a-35b694bf61fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6497,\n  \"fields\": [\n    {\n      \"column\": \"fixed acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.2964337577998153,\n        \"min\": 3.8,\n        \"max\": 15.9,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          7.15,\n          8.1,\n          7.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"volatile acidity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16463647408467877,\n        \"min\": 0.08,\n        \"max\": 1.58,\n        \"num_unique_values\": 187,\n        \"samples\": [\n          0.405,\n          0.21,\n          0.695\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"citric acid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14531786489759155,\n        \"min\": 0.0,\n        \"max\": 1.66,\n        \"num_unique_values\": 89,\n        \"samples\": [\n          0.1,\n          0.6,\n          0.37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"residual sugar\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.757803743147418,\n        \"min\": 0.6,\n        \"max\": 65.8,\n        \"num_unique_values\": 316,\n        \"samples\": [\n          18.95,\n          3.2,\n          9.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chlorides\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03503360137245907,\n        \"min\": 0.009,\n        \"max\": 0.611,\n        \"num_unique_values\": 214,\n        \"samples\": [\n          0.089,\n          0.217,\n          0.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"free sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.7493997720025,\n        \"min\": 1.0,\n        \"max\": 289.0,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          77.5,\n          65.0,\n          128.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total sulfur dioxide\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56.521854522630285,\n        \"min\": 6.0,\n        \"max\": 440.0,\n        \"num_unique_values\": 276,\n        \"samples\": [\n          14.0,\n          149.0,\n          227.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"density\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0029986730037190393,\n        \"min\": 0.98711,\n        \"max\": 1.03898,\n        \"num_unique_values\": 998,\n        \"samples\": [\n          0.9918,\n          0.99412,\n          0.99484\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.16078720210398764,\n        \"min\": 2.72,\n        \"max\": 4.01,\n        \"num_unique_values\": 108,\n        \"samples\": [\n          3.74,\n          3.17,\n          3.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sulphates\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14880587361449027,\n        \"min\": 0.22,\n        \"max\": 2.0,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          1.11,\n          1.56,\n          0.46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.192711748868981,\n        \"min\": 8.0,\n        \"max\": 14.9,\n        \"num_unique_values\": 111,\n        \"samples\": [\n          10.93333333,\n          9.7,\n          10.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 3,\n        \"max\": 9,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          5,\n          6,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f12b5f55-785b-4fdb-b5cd-6233ce7b5470\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f12b5f55-785b-4fdb-b5cd-6233ce7b5470')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f12b5f55-785b-4fdb-b5cd-6233ce7b5470 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f12b5f55-785b-4fdb-b5cd-6233ce7b5470');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-bd74b7df-78e9-46c0-a3ac-757056d81fc9\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd74b7df-78e9-46c0-a3ac-757056d81fc9')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-bd74b7df-78e9-46c0-a3ac-757056d81fc9 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  class  \n",
       "0      9.4        5      1  \n",
       "1      9.8        5      1  \n",
       "2      9.8        5      1  \n",
       "3      9.8        6      1  \n",
       "4      9.4        5      1  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/wine.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W6vbl9p3-k4e",
    "outputId": "498bb1bf-033d-45df-f29c-87df525e8ff3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device configuration: Cuda not available - check GPU configuration.\n",
      "Device configuration: Using cpu for training/inference\n",
      "Starting training...\n",
      "Training model with predictive objective\n",
      "Predictive - Epoch: 1, Step: 1, Total train loss: 1.0186, Validation criterion loss: 0.5321, Validation accuracy: 0.8231\n",
      "Predictive - Epoch: 2, Step: 2, Total train loss: 0.6489, Validation criterion loss: 0.4481, Validation accuracy: 0.81\n",
      "Predictive - Epoch: 3, Step: 3, Total train loss: 0.4932, Validation criterion loss: 0.3463, Validation accuracy: 0.8462\n",
      "Predictive - Epoch: 4, Step: 4, Total train loss: 0.4132, Validation criterion loss: 0.2788, Validation accuracy: 0.8892\n",
      "Predictive - Epoch: 5, Step: 5, Total train loss: 0.3539, Validation criterion loss: 0.2379, Validation accuracy: 0.92\n",
      "Predictive - Epoch: 6, Step: 6, Total train loss: 0.2994, Validation criterion loss: 0.1892, Validation accuracy: 0.9469\n",
      "Predictive - Epoch: 7, Step: 7, Total train loss: 0.2495, Validation criterion loss: 0.1644, Validation accuracy: 0.9485\n",
      "Predictive - Epoch: 8, Step: 8, Total train loss: 0.2179, Validation criterion loss: 0.1437, Validation accuracy: 0.9608\n",
      "Predictive - Epoch: 9, Step: 9, Total train loss: 0.1988, Validation criterion loss: 0.1337, Validation accuracy: 0.9608\n",
      "Predictive - Epoch: 10, Step: 10, Total train loss: 0.1808, Validation criterion loss: 0.1337, Validation accuracy: 0.9569\n",
      "Saving model to: runs/forest_cover/1725521397_forest_cover_predictive_model_final.pt\n",
      "Device configuration: Cuda not available - check GPU configuration.\n",
      "Device configuration: Using cpu for training/inference\n",
      "/content/tabnet/tabnet/train.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_params, model_state_dict = torch.load(\n",
      "TabNet accuracy: 0.957\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwoMXBlN-yEE"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LEwZmqyZR_TC"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/wine.csv /content/tabnet/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hp4qcBEUSMsn"
   },
   "outputs": [],
   "source": [
    "!pip install neptune-client==0.9.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F16debVNScai"
   },
   "outputs": [],
   "source": [
    "!pip install sparsemax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFX5rgmPRx5o",
    "outputId": "c426e2a9-bd0b-4d12-a626-8b613ffb3c99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device configuration: Cuda not available - check GPU configuration.\n",
      "Device configuration: Using cpu for training/inference\n",
      "Starting training...\n",
      "Training model with predictive objective\n",
      "Predictive - Epoch: 1, Step: 82, Total train loss: 0.2245, Validation criterion loss: 0.2147, Validation accuracy: 0.9108\n",
      "Predictive - Epoch: 2, Step: 164, Total train loss: 0.162, Validation criterion loss: 0.2831, Validation accuracy: 0.9292\n",
      "Predictive - Epoch: 3, Step: 246, Total train loss: 0.1112, Validation criterion loss: 0.2265, Validation accuracy: 0.9262\n",
      "Predictive - Epoch: 4, Step: 328, Total train loss: 0.0949, Validation criterion loss: 0.2372, Validation accuracy: 0.8992\n",
      "Predictive - Epoch: 5, Step: 410, Total train loss: 0.1109, Validation criterion loss: 0.328, Validation accuracy: 0.8838\n",
      "Predictive - Epoch: 6, Step: 492, Total train loss: 0.1038, Validation criterion loss: 0.2709, Validation accuracy: 0.8962\n",
      "Predictive - Epoch: 7, Step: 574, Total train loss: 0.1011, Validation criterion loss: 0.2778, Validation accuracy: 0.9023\n",
      "Predictive - Epoch: 8, Step: 656, Total train loss: 0.0926, Validation criterion loss: 0.286, Validation accuracy: 0.8923\n",
      "Predictive - Epoch: 9, Step: 738, Total train loss: 0.0919, Validation criterion loss: 0.0838, Validation accuracy: 0.9785\n",
      "Predictive - Epoch: 10, Step: 820, Total train loss: 0.0811, Validation criterion loss: 0.1946, Validation accuracy: 0.9415\n",
      "Saving model to: runs/forest_cover/1725189585_forest_cover_predictive_model_final.pt\n",
      "Device configuration: Cuda not available - check GPU configuration.\n",
      "Device configuration: Using cpu for training/inference\n",
      "/content/tabnet/tabnet/train.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_params, model_state_dict = torch.load(\n",
      "TabNet accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "!python main.py  #메인에 train valid 싹다 있음. #main.py 로거 싹다 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lxHkXboRv62"
   },
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1BzGOsmjvIv"
   },
   "source": [
    "### 2) TabNet 울렁증 버전\n",
    "\n",
    "---\n",
    "https://github.com/dreamquark-ai/tabnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nSG39zxgju1L",
    "outputId": "58e93d8b-c434-468f-a628-d4278d54046b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-tabnet\n",
      "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.3.2)\n",
      "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.4.0+cu121)\n",
      "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.5)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m717.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-4.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tz90wSGUM0Rk",
    "outputId": "18e6e437-ec27-4b33-beed-5c8151d910dc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.31352 | val_0_auc: 0.94458 |  0:00:01s\n",
      "epoch 1  | loss: 0.10203 | val_0_auc: 0.99114 |  0:00:02s\n",
      "epoch 2  | loss: 0.08551 | val_0_auc: 0.99484 |  0:00:03s\n",
      "epoch 3  | loss: 0.05907 | val_0_auc: 0.99412 |  0:00:05s\n",
      "epoch 4  | loss: 0.05207 | val_0_auc: 0.99656 |  0:00:06s\n",
      "epoch 5  | loss: 0.05124 | val_0_auc: 0.99835 |  0:00:08s\n",
      "epoch 6  | loss: 0.0477  | val_0_auc: 0.99935 |  0:00:10s\n",
      "epoch 7  | loss: 0.04294 | val_0_auc: 0.99938 |  0:00:12s\n",
      "epoch 8  | loss: 0.03777 | val_0_auc: 0.99966 |  0:00:13s\n",
      "epoch 9  | loss: 0.03198 | val_0_auc: 0.9995  |  0:00:15s\n",
      "epoch 10 | loss: 0.02692 | val_0_auc: 0.99944 |  0:00:16s\n",
      "epoch 11 | loss: 0.01987 | val_0_auc: 0.99919 |  0:00:17s\n",
      "epoch 12 | loss: 0.02677 | val_0_auc: 0.99908 |  0:00:18s\n",
      "epoch 13 | loss: 0.02331 | val_0_auc: 0.99847 |  0:00:19s\n",
      "epoch 14 | loss: 0.02111 | val_0_auc: 0.99874 |  0:00:19s\n",
      "epoch 15 | loss: 0.02229 | val_0_auc: 0.9972  |  0:00:20s\n",
      "epoch 16 | loss: 0.02312 | val_0_auc: 0.99699 |  0:00:20s\n",
      "epoch 17 | loss: 0.01808 | val_0_auc: 0.9982  |  0:00:21s\n",
      "epoch 18 | loss: 0.02353 | val_0_auc: 0.99716 |  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_val_0_auc = 0.99966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9938\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "# 특성과 레이블 분리\n",
    "X = df.iloc[:, :-1].values\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# 데이터셋을 훈련 세트와 테스트 세트로 분할\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "\n",
    "# TabNetClassifier 초기화 및 학습\n",
    "clf = TabNetClassifier()\n",
    "\n",
    "# GPU 모드일때\n",
    "#import torch\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#clf = TabNetClassifier(device_name=device.type)  # GPU 사용 설정\n",
    "\n",
    "clf.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    batch_size=256,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "\n",
    "# 예측\n",
    "preds = clf.predict(X_valid)\n",
    "\n",
    "# 성능 평가\n",
    "accuracy = accuracy_score(y_valid, preds)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENu-Vd2vjVTH"
   },
   "source": [
    "### 3) TabTransformer 오피셜이 공개되지 않은 버전\n",
    "\n",
    "---\n",
    "\n",
    "https://github.com/lucidrains/tab-transformer-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cso1MB3ajYSD",
    "outputId": "1ef73cc8-75da-4ece-ee64-352276bfaaa5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tab-transformer-pytorch\n",
      "  Downloading tab_transformer_pytorch-0.3.0-py3-none-any.whl.metadata (690 bytes)\n",
      "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.10/dist-packages (from tab-transformer-pytorch) (0.8.0)\n",
      "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from tab-transformer-pytorch) (2.4.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->tab-transformer-pytorch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->tab-transformer-pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->tab-transformer-pytorch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->tab-transformer-pytorch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->tab-transformer-pytorch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->tab-transformer-pytorch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->tab-transformer-pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->tab-transformer-pytorch) (1.3.0)\n",
      "Downloading tab_transformer_pytorch-0.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: tab-transformer-pytorch\n",
      "Successfully installed tab-transformer-pytorch-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BvES6NiQ9iRB",
    "outputId": "058b69b8-e9f7-457a-bbb2-5fe0e0d72d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 0.7227\n",
      "Epoch 10/50, Loss: 0.6908\n",
      "Epoch 15/50, Loss: 0.6619\n",
      "Epoch 20/50, Loss: 0.6333\n",
      "Epoch 25/50, Loss: 0.6018\n",
      "Epoch 30/50, Loss: 0.5647\n",
      "Epoch 35/50, Loss: 0.5207\n",
      "Epoch 40/50, Loss: 0.4710\n",
      "Epoch 45/50, Loss: 0.4176\n",
      "Epoch 50/50, Loss: 0.3623\n",
      "Valid Accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('wine.csv')\n",
    "\n",
    "# 독립 변수와 종속 변수 분리\n",
    "X = df.drop('class', axis=1).values  # class 열 제외한 나머지 열 사용\n",
    "y = df['class'].values  # 0 또는 1로 레이블링된 와인 품질\n",
    "\n",
    "# 데이터셋 분리 (훈련/테스트)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# TabTransformer 모델 정의\n",
    "model = TabTransformer(\n",
    "    categories=[],\n",
    "    num_continuous=X_train_tensor.shape[1],  # 연속형 변수의 개수\n",
    "    dim=32,  # 모델 차원\n",
    "    dim_out=1,  # 이진 분류 출력\n",
    "    depth=6,  # 모델 깊이\n",
    "    heads=8,  # 멀티헤드 어텐션 헤드 수\n",
    "    attn_dropout=0.1,  # 어텐션 드롭아웃\n",
    "    ff_dropout=0.1,  # 피드포워드 드롭아웃\n",
    "    mlp_hidden_mults=(4, 2),  # MLP의 히든 레이어 크기 비율\n",
    "    mlp_act=nn.ReLU(),  # MLP의 활성화 함수\n",
    ")\n",
    "\n",
    "# 모델 학습 준비\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 예측 및 손실 계산\n",
    "    y_pred = model(torch.empty((X_train_tensor.shape[0], 0), dtype=torch.int64), X_train_tensor)  # 범주형 변수가 없으므로 비어있는 텐서를 줘야함.\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "\n",
    "    # 역전파 및 최적화\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 5 == 0 :\n",
    "      print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.empty((X_train_tensor.shape[0], 0), dtype=torch.int64), X_test_tensor)\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    y_pred_class = (y_pred > 0.5).float()\n",
    "\n",
    "    accuracy = (y_pred_class == y_test_tensor).float().mean()\n",
    "    print(f'Valid Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3srp0vERA73j"
   },
   "source": [
    "### 3) TabTransformer 오피셜이 공개되지 않은 버전 - 타이타닉\n",
    "\n",
    "---\n",
    "\n",
    "https://github.com/lucidrains/tab-transformer-pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgKdicetA4em",
    "outputId": "351a4e0d-642d-4a54-d666-c2f8abd3ee83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Loss: 0.4601\n",
      "Epoch 10/50, Loss: 0.4423\n",
      "Epoch 15/50, Loss: 0.4370\n",
      "Epoch 20/50, Loss: 0.4333\n",
      "Epoch 25/50, Loss: 0.4311\n",
      "Epoch 30/50, Loss: 0.4291\n",
      "Epoch 35/50, Loss: 0.4299\n",
      "Epoch 40/50, Loss: 0.4284\n",
      "Epoch 45/50, Loss: 0.4265\n",
      "Epoch 50/50, Loss: 0.4268\n",
      "Valid Accuracy: 0.7762\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tab_transformer_pytorch import TabTransformer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# 필요한 열 선택 및 전처리\n",
    "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked']]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 범주형 변수 인코딩\n",
    "label_encoders = {}\n",
    "for col in ['Pclass', 'Sex', 'Embarked']:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "\n",
    "X = df.drop('Survived', axis=1).values  # 'Survived' 열 제외한 나머지 열 사용\n",
    "y = df['Survived'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 연속형 변수만 정규화 적용\n",
    "scaler = StandardScaler()\n",
    "X_train[:, [2, 3]] = scaler.fit_transform(X_train[:, [2, 3]])  # 연속형 변수 (Age, Fare)만 정규화\n",
    "X_test[:, [2, 3]] = scaler.transform(X_test[:, [2, 3]])\n",
    "\n",
    "X_train_categ = torch.tensor(X_train[:, [0, 1, 4]], dtype=torch.int64)  # 범주형 변수 (Pclass, Sex, Embarked)\n",
    "X_train_cont = torch.tensor(X_train[:, [2, 3]], dtype=torch.float32)  # 연속형 변수 (Age, Fare)\n",
    "X_test_categ = torch.tensor(X_test[:, [0, 1, 4]], dtype=torch.int64)\n",
    "X_test_cont = torch.tensor(X_test[:, [2, 3]], dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# TabTransformer 모델 정의\n",
    "model = TabTransformer(\n",
    "    categories=(3, 2, 3),  # Pclass, Sex, Embarked의 고유 값 개수\n",
    "    num_continuous=X_train_cont.shape[1],  # 연속형 변수의 개수\n",
    "    dim=32,  # 모델 차원\n",
    "    dim_out=1,  # 이진 분류 출력\n",
    "    depth=6,  # 모델 깊이\n",
    "    heads=8,  # 멀티헤드 어텐션 헤드 수\n",
    "    attn_dropout=0.1,  # 어텐션 드롭아웃\n",
    "    ff_dropout=0.1,  # 피드포워드 드롭아웃\n",
    "    mlp_hidden_mults=(4, 2),  # MLP의 히든 레이어 크기 비율\n",
    "    mlp_act=nn.ReLU(),  # MLP의 활성화 함수\n",
    ")\n",
    "\n",
    "# 모델 학습 준비\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 루프\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 예측 및 손실 계산\n",
    "    y_pred = model(X_train_categ, X_train_cont)  # 범주형 및 연속형 변수 모두 전달\n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "\n",
    "    # 역전파 및 최적화\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_categ, X_test_cont)\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    y_pred_class = (y_pred > 0.5).float()\n",
    "\n",
    "    accuracy = (y_pred_class == y_test_tensor).float().mean()\n",
    "    print(f'Valid Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZk0HEkbTHJz"
   },
   "source": [
    "# ★ 실무 실습) 배포 받은 \"Loan-Status-Prediction\"를 통해 자신만의 코드를 작성 해보자\n",
    "\n",
    "1. 대출자가 상환할 가능성이 있는지를 분류하는 모델을 만들 것이다.\n",
    "2. 하나의 포트폴리오가 될 수 있도록 시각화와 통계를 자유롭게 작성해보자.\n",
    "3. 이제까지 배운 분류 방법들을 통해 적용해보자.\n",
    "4. 모든 기법을 다 쓰는 것보다는 필요 할 것 같다는 코드만 작성해보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24koOX2QXA8x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPgEEjb4XCov"
   },
   "source": [
    "###  위 코드에서 어떤 기법들을 썼었고 왜 그것들을 썼는지 서술 하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmLB0oyrXB9S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
